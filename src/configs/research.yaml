# Research configuration for sentence transformer fine-tuning

# Model configuration
model:
  name: "sentence-transformers/all-mpnet-base-v2"
  max_length: 512
  dropout: 0.1

# Training configuration
training:
  batch_size: 16
  learning_rate: 2e-5
  num_epochs: 10
  warmup_steps: 100
  weight_decay: 0.01
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1

# Data configuration
data:
  train_file: "data/train.csv"
  val_file: "data/val.csv"
  test_file: "data/test.csv"
  text_column: "text"
  label_column: "label"
  val_split: 0.2

# Hugging Face configuration
huggingface:
  repo_name: "your-org/sentence-transformer-research"
  push_to_hub: true
  private: false

# Logging configuration
logging:
  use_wandb: true
  project_name: "sentence-transformer-research"
  log_every_n_steps: 10

# Hardware configuration
hardware:
  num_workers: 4
  precision: "32"
  devices: "auto"
