# Production configuration for sentence transformer fine-tuning

# Model configuration
model:
  name: "sentence-transformers/all-MiniLM-L6-v2"
  max_length: 512
  dropout: 0.1

# Training configuration
training:
  batch_size: 32
  learning_rate: 1e-5
  num_epochs: 5
  warmup_steps: 500
  weight_decay: 0.01
  gradient_clip_val: 1.0
  accumulate_grad_batches: 2

# Data configuration
data:
  train_file: "data/train.csv"
  val_file: "data/val.csv"
  test_file: "data/test.csv"
  text_column: "text"
  label_column: "label"
  val_split: 0.15

# Hugging Face configuration
huggingface:
  repo_name: "your-org/sentence-transformer-classifier"
  push_to_hub: true
  private: false

# Logging configuration
logging:
  use_wandb: true
  project_name: "sentence-transformer-production"
  log_every_n_steps: 25

# Hardware configuration
hardware:
  num_workers: 8
  precision: "16-mixed"
  devices: "auto"
